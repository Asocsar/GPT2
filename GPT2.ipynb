{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numpy import pi, sqrt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "physical_devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len, vocab_size, embedding_dim):\n",
    "        super(PositionEmbeding, self).__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(max_len, embedding_dim, mask_zero=True)\n",
    "        self.start = 1\n",
    "    \n",
    "    def get_token_embedding(self):\n",
    "        return self.token_emb.weights\n",
    "    \n",
    "    def set_start(self, N):\n",
    "        self.start = N\n",
    "    \n",
    "    def position_embedding(self, x):\n",
    "        batch_length = tf.shape(x)[1] \n",
    "        batch_size = tf.shape(x)[0]\n",
    "        \n",
    "        pos = tf.reshape(tf.tile(tf.range(self.start, batch_length + self.start), [batch_size]),\n",
    "                                       [batch_size, batch_length])\n",
    "        pos = tf.cast(pos, tf.int32)\n",
    "        invalid_pos = tf.cast(tf.not_equal(x, 0), tf.int32)\n",
    "        pos *= invalid_pos\n",
    "        \n",
    "        return self.pos_emb(pos)\n",
    "        \n",
    "    def token_embedding(self, x):\n",
    "        \n",
    "        words = self.token_emb(x)\n",
    "        return words\n",
    "    \n",
    "    def call(self, input):\n",
    "        positions = self.position_embedding(input)\n",
    "        words = self.token_embedding(input)\n",
    "        return words + positions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GELU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GELU(x):\n",
    "    return 0.5*x*(1+tf.tanh(sqrt(2/pi)*(x+0.044715*tf.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch = tf.shape(x)[0]\n",
    "        *prev, embedding_dim = x.shape.as_list()\n",
    "        x = tf.reshape(x, [batch] + [-1] + [self.num_heads, embedding_dim//self.num_heads])\n",
    "        return tf.transpose(x, [0, 2, 1, 3])\n",
    "    \n",
    "    def merge_heads(self, x):\n",
    "        batch = tf.shape(x)[0]\n",
    "        x = tf.transpose(x, [0, 2, 1, 3])\n",
    "        *prev, heads, sub_dim = x.shape.as_list()\n",
    "        x = tf.reshape(x, [batch] + [-1] + [heads*sub_dim])\n",
    "        return x\n",
    "    \n",
    "    def call(self, input):\n",
    "        \n",
    "        input, past, mask = input\n",
    "        \n",
    "        #Para la dimension 3, que ahora mismo tenemos [batch, seq_len, embedding*3]\n",
    "        # dividimos en 3 partes para generar query, key y value que al mismo tiempo\n",
    "        # dividimos en los heads que queramos\n",
    "        q, k, v = map(self.split_heads, tf.split(input, 3, axis=2))\n",
    "        \n",
    "        \n",
    "        #present = tf.stack([k,v], axis=1)\n",
    "        if past is not None:\n",
    "            pk, pv = tf.unstack(past, axis=1)\n",
    "            k = tf.concat([pk,k], axis=-2)\n",
    "            v = tf.concat([pv,v], axis=-2)\n",
    "        \n",
    "        present = tf.stack([k,v], axis=1)\n",
    "        \n",
    "        #Por las siguientes multiplicaciones tendremos que w tiene dimensiones\n",
    "        #[batch, heads, dst_sec, orig_sec] es decir tenemos para todos los elementos del batch\n",
    "        #y para todos los heads una matriz donde cada columna y fila son palabras, las diagonales\n",
    "        #son las mismas palabras y el valor es la \"puntuaci贸n\" o relaci贸n que esta palabra tiene\n",
    "        #con la otra\n",
    "        w = tf.matmul(q, k, transpose_b=True)\n",
    "        w /= tf.math.rsqrt(tf.cast(v.shape.as_list()[-1], w.dtype))\n",
    "        \n",
    "        if mask is not None:\n",
    "            w += (mask * -1e9)\n",
    "        \n",
    "        #Aplicamos la mascara, softmax para regular entre 0 y 1 y finalmente multiplicamos los valores\n",
    "        #de query y key por value\n",
    "        #w = self.apply_mask(w)\n",
    "        w = tf.nn.softmax(w, axis=-1)\n",
    "        output = tf.matmul(w, v)\n",
    "        \n",
    "        return self.merge_heads(output), present\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.MultiHeadAttention = MultiHeadAttention(num_heads=num_heads)\n",
    "        \n",
    "        self.LayerNorm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.LayerNorm2 = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "        self.GELU = tf.keras.layers.Activation(GELU)\n",
    "        \n",
    "        self.conv1D_1 = tf.keras.layers.Conv1D(d_model*3, 1)\n",
    "        self.conv1D_2 = tf.keras.layers.Conv1D(d_model*4, 1)\n",
    "        \n",
    "        self.conv1D_3 = tf.keras.layers.Conv1D(d_model, 1)\n",
    "        self.conv1D_4 = tf.keras.layers.Conv1D(d_model, 1)\n",
    "\n",
    "    \n",
    "    def call(self, input):\n",
    "        x, past, mask = input\n",
    "        x_norm1 = self.LayerNorm1(x)\n",
    "        x_conv1 = self.conv1D_1(x_norm1)\n",
    "        x_attn, present = self.MultiHeadAttention((x_conv1, past, mask))\n",
    "        x_conv3 = self.conv1D_3(x_attn)\n",
    "        \n",
    "        x = tf.keras.layers.Add()([x_conv3,x])\n",
    "        \n",
    "        x_norm2 = self.LayerNorm2(x)\n",
    "        x_conv2 = self.conv1D_2(x_norm2)\n",
    "        \n",
    "        #https://mlfromscratch.com/activation-functions-explained/#/\n",
    "        x_gelu = self.GELU(x_conv2)\n",
    "        \n",
    "        # Hay paginas que hablan de dos densas, pero en el codigo se utiliza otra cosa WTF??\n",
    "        # https://www.reddit.com/r/MachineLearning/comments/b1c6sn/d_is_gpt2_source_code_publically_available/eilbqas/\n",
    "        # en ese enlace se menciona posible reduccion del codigo?\n",
    "        \n",
    "        # Si ponemos dos sensas no llegamos a lo pedido (en cuanto a pesos) indicados en el paper\n",
    "        # entonces es menos potente?? O estan contando pesos fuera del model, pero no tiene sentido\n",
    "        \n",
    "        \n",
    "        \n",
    "        x_conv4 = self.conv1D_4(x_gelu)\n",
    "        \n",
    "        x = tf.keras.layers.Add()([x_conv4,x])\n",
    "        return x, present\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(tf.keras.Model):\n",
    "    def __init__(self, num_layers, num_heads, d_model, vocab_size, max_len):\n",
    "        super(GPT2, self).__init__()\n",
    "        self.DecoderBlocks = [DecoderBlock(num_heads, d_model) for _ in range(num_layers)]\n",
    "        self.PosEmb = PositionEmbeding(max_len, vocab_size, d_model)\n",
    "        self.Norm = tf.keras.layers.LayerNormalization()\n",
    "        self.vocab = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.pasts = list([None] * num_layers)\n",
    "        self.conditioned = False\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def reset_past_status(self):\n",
    "        self.pasts = [None] * num_layers\n",
    "        self.conditioned = False\n",
    "    \n",
    "    def generate_next_output(self, X):\n",
    "        start = 1 if not self.conditioned else tf.shape(self.pasts)[-2]\n",
    "        self.PosEmb.set_start(start)\n",
    "        \n",
    "        output, present = self.call(X)\n",
    "        self.pasts = present\n",
    "        self.conditioned = True\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask\n",
    "    \n",
    "    def create_paddding_mask(self, inp):\n",
    "        padded = tf.cast(tf.math.equal(inp, 0), tf.float32)\n",
    "        return padded[:, tf.newaxis, tf.newaxis, :]\n",
    "    \n",
    "    def create_mask(self, x):\n",
    "        matrix_dim = tf.shape(x)[-1]\n",
    "        \n",
    "        mask_ahead = self.create_look_ahead_mask(matrix_dim)\n",
    "        mask_padd = self.create_paddding_mask(x)\n",
    "        \n",
    "        mask = tf.maximum(mask_padd, mask_ahead)\n",
    "        return mask\n",
    "    \n",
    "    def call(self, input):\n",
    "        presents = []\n",
    "        \n",
    "        assert len(self.pasts) == self.num_layers\n",
    "        \n",
    "        mask = self.create_mask(input)\n",
    "        x = self.PosEmb(input)\n",
    "        \n",
    "        for DecoderB, past in zip(self.DecoderBlocks, self.pasts):\n",
    "            x, present = DecoderB((x, past, mask))\n",
    "            presents.append(present)\n",
    "            \n",
    "        x = self.Norm(x)\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = tf.reshape(x, [-1, self.d_model])\n",
    "        x = tf.matmul(x, self.PosEmb.get_token_embedding(), transpose_b=True)\n",
    "        logits = tf.reshape(x, [-1, seq_len, self.vocab])\n",
    "        return logits, presents\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        gc.collect()\n",
    "        x, y = data\n",
    "        x = tf.cast(x, tf.int32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred, _ = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        perplexity = tf.exp(loss)\n",
    "        \n",
    "        return {\"Loss\": loss, \"Perplexity\": perplexity}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "  \n",
    "        y_pred, _ = self(x, training=False)\n",
    "\n",
    "        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        perplexity = tf.exp(loss)\n",
    "\n",
    "        return {\"Loss\": loss, \"Perplexity\": perplexity}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "Root = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_originalData = Root + \"/Dataset\"\n",
    "all_files = [os.path.join(path_originalData,file) for file in os.listdir(path_originalData) if '.txt' in file[-4:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix posible problems in Unicode Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ftfy import fix_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "procesed_path = Root + \"/Processed.txt\"\n",
    "writer = open(procesed_path, \"w\")\n",
    "for file in all_files:\n",
    "    f = open(file, \"r\")\n",
    "    writer.writelines([fix_text(line, normalization=\"NFKC\") for line in f.readlines()])\n",
    "    f.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = Counter()\n",
    "with open(procesed_path, 'r') as f:\n",
    "    for line in f:\n",
    "        token_count.update(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_path = Root + \"/Counter.txt\"\n",
    "with open(counter_path, 'w', newline='') as f:\n",
    "    output = csv.writer(f, delimiter='\\t')\n",
    "    for word in token_count:\n",
    "        output.writerow([word, token_count[word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libreria que nos crea la codificaci贸n byte Pair Encoding (el vocab_size es a nuestra elecci贸n)\n",
    "Model_path = Root + \"/BPE_Model\"\n",
    "vocab_size = 24512 #int(len(token_count)/4)\n",
    "\n",
    "spmcmd = '--input={spm_input} --model_prefix={spm_model} --input_format=tsv --vocab_size={vocab_size} --user_defined_symbols=[SEP],[BOS],[EOS] --hard_vocab_limit=false --model_type=bpe --pad_id=0 --unk_id=1 --bos_id=-1 --eos_id=-1 --pad_piece=[PAD] --unk_piece=[UNK]'.format(\n",
    "spm_input=counter_path, spm_model=Model_path, vocab_size=vocab_size)\n",
    "spm.SentencePieceTrainer.train(spmcmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = spm.SentencePieceProcessor()\n",
    "s.Load(Model_path + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 3\n",
    "EOS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "count = 0\n",
    "\n",
    "min_seq_len = 15\n",
    "max_seq_len = 128\n",
    "\n",
    "with open(procesed_path, 'r') as f:\n",
    "    for line in f:\n",
    "        encod = s.encode_as_ids(line)\n",
    "        if max_seq_len > len(encod) > min_seq_len:\n",
    "            dataset += [[[BOS]+ encod, encod + [EOS]]]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = (85 / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [X[0] for X in dataset]\n",
    "Y = [Y[1] for Y in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=1-train_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(max([len(x) for x in X_train]), max([len(x) for x in X_test])) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(len(X_train)):\\n    X_train[i] += [0 for _ in range(max_len-len(X_train[i]))]\\n    y_train[i] += [0 for _ in range(max_len-len(y_train[i]))]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i in range(len(X_train)):\n",
    "    X_train[i] += [0 for _ in range(max_len-len(X_train[i]))]\n",
    "    y_train[i] += [0 for _ in range(max_len-len(y_train[i]))]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(len(X_test)):\\n    X_test[i] += [0 for _ in range(max_len-len(X_test[i]))]\\n    y_test[i] += [0 for _ in range(max_len-len(y_test[i]))]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i in range(len(X_test)):\n",
    "    X_test[i] += [0 for _ in range(max_len-len(X_test[i]))]\n",
    "    y_test[i] += [0 for _ in range(max_len-len(y_test[i]))]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = tf.data.Dataset.from_tensor_slices((tf.constant(X_train), tf.constant(y_train))).batch(1, drop_remainder=True)\n",
    "#test_dataset  = tf.data.Dataset.from_tensor_slices((tf.constant(X_test), tf.constant(y_test))).batch(1, drop_remainder=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((tf.ragged.constant(X_train), tf.ragged.constant(y_train))).map(lambda x,y: (x,y)).padded_batch(BATCH_SIZE)\n",
    "test_dataset  = tf.data.Dataset.from_tensor_slices((tf.ragged.constant(X_test), tf.ragged.constant(y_test))).map(lambda x,y: (x,y)).padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 8\n",
    "num_heads = 8\n",
    "d_model = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = GPT2(num_layers, num_heads, d_model, vocab_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(y_true, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "    \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(y_true, y_pred)\n",
    "    \n",
    "    mask = tf.cast(mask, loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "\n",
    "    loss = tf.reduce_sum(loss, axis=1)\n",
    "    average_loss = tf.reduce_mean(loss / tf.reduce_sum(mask, axis=1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return tf.reduce_mean(loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "901/901 [==============================] - 97s 102ms/step - Loss: 252.1590 - Perplexity: inf - val_Loss: 186.3972 - val_Perplexity: inf\n",
      "Epoch 2/50\n",
      "901/901 [==============================] - 84s 93ms/step - Loss: 250.4483 - Perplexity: inf - val_Loss: 189.2562 - val_Perplexity: inf\n",
      "Epoch 3/50\n",
      "901/901 [==============================] - 84s 93ms/step - Loss: 236.9479 - Perplexity: inf - val_Loss: 177.6967 - val_Perplexity: inf\n",
      "Epoch 4/50\n",
      "901/901 [==============================] - 84s 93ms/step - Loss: 236.1767 - Perplexity: inf - val_Loss: 209.5417 - val_Perplexity: inf\n",
      "Epoch 5/50\n",
      "901/901 [==============================] - 84s 93ms/step - Loss: 242.1771 - Perplexity: inf - val_Loss: 176.5429 - val_Perplexity: inf\n",
      "Epoch 6/50\n",
      "901/901 [==============================] - 84s 93ms/step - Loss: 252.6380 - Perplexity: inf - val_Loss: 178.5452 - val_Perplexity: inf\n",
      "Epoch 7/50\n",
      "901/901 [==============================] - 84s 93ms/step - Loss: 227.6333 - Perplexity: inf - val_Loss: 172.8761 - val_Perplexity: inf\n",
      "Epoch 8/50\n",
      "901/901 [==============================] - 84s 93ms/step - Loss: 228.2717 - Perplexity: inf - val_Loss: 170.8390 - val_Perplexity: inf\n",
      "Epoch 9/50\n",
      "901/901 [==============================] - 85s 94ms/step - Loss: 242.2926 - Perplexity: inf - val_Loss: 191.0838 - val_Perplexity: inf\n",
      "Epoch 10/50\n",
      "901/901 [==============================] - 85s 94ms/step - Loss: 270.2063 - Perplexity: inf - val_Loss: 200.4786 - val_Perplexity: inf\n",
      "Epoch 11/50\n",
      "901/901 [==============================] - 85s 94ms/step - Loss: 248.9995 - Perplexity: inf - val_Loss: 187.8033 - val_Perplexity: inf\n",
      "Epoch 12/50\n",
      "901/901 [==============================] - 85s 95ms/step - Loss: 247.5286 - Perplexity: inf - val_Loss: 193.2004 - val_Perplexity: inf\n",
      "Epoch 13/50\n",
      "901/901 [==============================] - 85s 94ms/step - Loss: 258.3083 - Perplexity: inf - val_Loss: 195.5856 - val_Perplexity: inf\n",
      "Epoch 14/50\n",
      "901/901 [==============================] - 85s 94ms/step - Loss: 257.1194 - Perplexity: inf - val_Loss: 203.1742 - val_Perplexity: inf\n",
      "Epoch 15/50\n",
      "901/901 [==============================] - 85s 95ms/step - Loss: 253.2779 - Perplexity: inf - val_Loss: 189.3954 - val_Perplexity: inf\n",
      "Epoch 16/50\n",
      "901/901 [==============================] - 85s 95ms/step - Loss: 241.5147 - Perplexity: inf - val_Loss: 187.6595 - val_Perplexity: inf\n",
      "Epoch 17/50\n",
      "864/901 [===========================>..] - ETA: 3s - Loss: 240.2219 - Perplexity: inf"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1dbb43a57390>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = gpt2.fit(train_dataset, validation_data=test_dataset, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        return logits\n",
    "\n",
    "    values, _ = tf.nn.top_k(logits, k=k)\n",
    "    min_values = values[:, -1]\n",
    "\n",
    "    return tf.where(\n",
    "        logits < min_values,\n",
    "        tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "        logits\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(logits, temperature=1, top_k=50):\n",
    "    logits = logits[:, -1, :]/tf.cast(temperature, tf.float32)\n",
    "    #print(logits)\n",
    "    logits = top_k_logits(logits, k=top_k)\n",
    "    #print(logits)\n",
    "    sample = tf.random.categorical(logits, num_samples=1, dtype=tf.int32)\n",
    "    #sample = tf.nn.softmax(logits)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puta = ''\n",
    "def generator_text(initial_sentence, model, seq_len, temp=1, k=10, clear_status=True):\n",
    "    global puta\n",
    "    if clear_status:\n",
    "        gpt2.reset_past_status()\n",
    "\n",
    "    context = tf.expand_dims(([BOS] + s.encode_as_ids(initial_sentence)), 0)\n",
    "    prev = context\n",
    "    output = context\n",
    "    j = 0\n",
    "    for i in range(seq_len):\n",
    "        logits = model.generate_next_output(prev)\n",
    "        puta = logits\n",
    "        #print(\"original\", logits)\n",
    "        sample = extract_data(logits)\n",
    "        #print(sample)\n",
    "        if tf.equal(sample, EOS):\n",
    "            print(j, \"END\")\n",
    "            break\n",
    "        output = tf.concat([output, sample], axis=-1)\n",
    "        prev = sample\n",
    "        j += 1\n",
    "    print(output)\n",
    "    result = tf.squeeze(output, axis=0)\n",
    "    pred = [int(i) for i in result]\n",
    "    generated_seq = s.decode_ids(pred[1:])\n",
    "    generated_seq = generated_seq.replace(\"[SEP]\", \"\").strip()\n",
    "    generated_seq = ' '.join(generated_seq.split())\n",
    "    return generated_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Initial_Sentence = \"i have a friend called \"\n",
    "generator_text(Initial_Sentence, gpt2, 50, temp=0.7, k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss([y_train[0]],  out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "Loss([y_train[0][:4]], out[:, :4, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss([y_train[0][:]], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [x for x in range(98)]\n",
    "l2 = [0 for x in range(98)]\n",
    "m = tf.math.logical_not(tf.math.equal(tf.constant([y_train[0]]), 0))\n",
    "tf.constant([y_train[0]])[m]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.reset_past_status()\n",
    "out = gpt2.generate_next_output(tf.constant([X_train[0]]))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.argmax(out, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant([X_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant([y_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in train_dataset.take(1):\n",
    "    print(ex[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = next(train_dataset.as_numpy_iterator())[0]\n",
    "Y = next(train_dataset.as_numpy_iterator())[1]\n",
    "print(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.math.logical_not(tf.math.equal(a, 0))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tf.shape(a)[0]\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = a[m]\n",
    "np.reshape(x, (batch, 98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
