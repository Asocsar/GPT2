{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numpy import pi, sqrt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "physical_devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len, vocab_size, embedding_dim):\n",
    "        super(PositionEmbeding, self).__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(max_len, embedding_dim, mask_zero=True)\n",
    "        self.start = 1\n",
    "    \n",
    "    def get_token_embedding(self):\n",
    "        return self.token_emb.weights\n",
    "    \n",
    "    def set_start(self, N):\n",
    "        self.start = N\n",
    "    \n",
    "    def position_embedding(self, x):\n",
    "        batch_length = tf.shape(x)[1] \n",
    "        batch_size = tf.shape(x)[0]\n",
    "        \n",
    "        pos = tf.reshape(tf.tile(tf.range(self.start, batch_length + self.start), [batch_size]),\n",
    "                                       [batch_size, batch_length])\n",
    "        pos = tf.cast(pos, tf.int32)\n",
    "        invalid_pos = tf.cast(tf.not_equal(x, 0), tf.int32)\n",
    "        pos *= invalid_pos\n",
    "        \n",
    "        return self.pos_emb(pos)\n",
    "        \n",
    "    def token_embedding(self, x):\n",
    "        \n",
    "        words = self.token_emb(x)\n",
    "        return words\n",
    "    \n",
    "    def call(self, input):\n",
    "        positions = self.position_embedding(input)\n",
    "        words = self.token_embedding(input)\n",
    "        return words + positions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GELU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GELU(x):\n",
    "    return 0.5*x*(1+tf.tanh(sqrt(2/pi)*(x+0.044715*tf.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch = tf.shape(x)[0]\n",
    "        *prev, embedding_dim = x.shape.as_list()\n",
    "        x = tf.reshape(x, [batch] + [-1] + [self.num_heads, embedding_dim//self.num_heads])\n",
    "        return tf.transpose(x, [0, 2, 1, 3])\n",
    "    \n",
    "    def merge_heads(self, x):\n",
    "        batch = tf.shape(x)[0]\n",
    "        x = tf.transpose(x, [0, 2, 1, 3])\n",
    "        *prev, heads, sub_dim = x.shape.as_list()\n",
    "        x = tf.reshape(x, [batch] + [-1] + [heads*sub_dim])\n",
    "        return x\n",
    "    \n",
    "    def call(self, input):\n",
    "        \n",
    "        input, past, mask = input\n",
    "        \n",
    "        #Para la dimension 3, que ahora mismo tenemos [batch, seq_len, embedding*3]\n",
    "        # dividimos en 3 partes para generar query, key y value que al mismo tiempo\n",
    "        # dividimos en los heads que queramos\n",
    "        q, k, v = map(self.split_heads, tf.split(input, 3, axis=2))\n",
    "        \n",
    "        \n",
    "        #present = tf.stack([k,v], axis=1)\n",
    "        if past is not None:\n",
    "            pk, pv = tf.unstack(past, axis=1)\n",
    "            k = tf.concat([pk,k], axis=-2)\n",
    "            v = tf.concat([pv,v], axis=-2)\n",
    "        \n",
    "        present = tf.stack([k,v], axis=1)\n",
    "        \n",
    "        #Por las siguientes multiplicaciones tendremos que w tiene dimensiones\n",
    "        #[batch, heads, dst_sec, orig_sec] es decir tenemos para todos los elementos del batch\n",
    "        #y para todos los heads una matriz donde cada columna y fila son palabras, las diagonales\n",
    "        #son las mismas palabras y el valor es la \"puntuación\" o relación que esta palabra tiene\n",
    "        #con la otra\n",
    "        w = tf.matmul(q, k, transpose_b=True)\n",
    "        w /= tf.math.rsqrt(tf.cast(v.shape.as_list()[-1], w.dtype))\n",
    "        \n",
    "        if mask is not None:\n",
    "            w += (mask * -1e9)\n",
    "        \n",
    "        #Aplicamos la mascara, softmax para regular entre 0 y 1 y finalmente multiplicamos los valores\n",
    "        #de query y key por value\n",
    "        #w = self.apply_mask(w)\n",
    "        w = tf.nn.softmax(w, axis=-1)\n",
    "        output = tf.matmul(w, v)\n",
    "        \n",
    "        return self.merge_heads(output), present\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.MultiHeadAttention = MultiHeadAttention(num_heads=num_heads)\n",
    "        \n",
    "        self.LayerNorm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.LayerNorm2 = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "        self.GELU = tf.keras.layers.Activation(GELU)\n",
    "        \n",
    "        self.conv1D_1 = tf.keras.layers.Conv1D(d_model*3, 1)\n",
    "        self.conv1D_2 = tf.keras.layers.Conv1D(d_model*4, 1)\n",
    "        \n",
    "        self.conv1D_3 = tf.keras.layers.Conv1D(d_model, 1)\n",
    "        self.conv1D_4 = tf.keras.layers.Conv1D(d_model, 1)\n",
    "\n",
    "    \n",
    "    def call(self, input):\n",
    "        x, past, mask = input\n",
    "        x_norm1 = self.LayerNorm1(x)\n",
    "        x_conv1 = self.conv1D_1(x_norm1)\n",
    "        x_attn, present = self.MultiHeadAttention((x_conv1, past, mask))\n",
    "        x_conv3 = self.conv1D_3(x_attn)\n",
    "        \n",
    "        x = tf.keras.layers.Add()([x_conv3,x])\n",
    "        \n",
    "        x_norm2 = self.LayerNorm2(x)\n",
    "        x_conv2 = self.conv1D_2(x_norm2)\n",
    "        \n",
    "        #https://mlfromscratch.com/activation-functions-explained/#/\n",
    "        x_gelu = self.GELU(x_conv2)\n",
    "        \n",
    "        # Hay paginas que hablan de dos densas, pero en el codigo se utiliza otra cosa WTF??\n",
    "        # https://www.reddit.com/r/MachineLearning/comments/b1c6sn/d_is_gpt2_source_code_publically_available/eilbqas/\n",
    "        # en ese enlace se menciona posible reduccion del codigo?\n",
    "        \n",
    "        # Si ponemos dos sensas no llegamos a lo pedido (en cuanto a pesos) indicados en el paper\n",
    "        # entonces es menos potente?? O estan contando pesos fuera del model, pero no tiene sentido\n",
    "        \n",
    "        \n",
    "        \n",
    "        x_conv4 = self.conv1D_4(x_gelu)\n",
    "        \n",
    "        x = tf.keras.layers.Add()([x_conv4,x])\n",
    "        return x, present\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(tf.keras.Model):\n",
    "    def __init__(self, num_layers, num_heads, d_model, vocab_size, max_len):\n",
    "        super(GPT2, self).__init__()\n",
    "        self.DecoderBlocks = [DecoderBlock(num_heads, d_model) for _ in range(num_layers)]\n",
    "        self.PosEmb = PositionEmbeding(max_len, vocab_size, d_model)\n",
    "        self.Norm = tf.keras.layers.LayerNormalization()\n",
    "        self.vocab = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.pasts = list([None] * num_layers)\n",
    "        self.conditioned = False\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def reset_past_status(self):\n",
    "        self.pasts = [None] * num_layers\n",
    "        self.conditioned = False\n",
    "    \n",
    "    def generate_next_output(self, X):\n",
    "        start = 1 if not self.conditioned else tf.shape(self.pasts)[-2]\n",
    "        self.PosEmb.set_start(start)\n",
    "        \n",
    "        output, present = self.call(X)\n",
    "        self.pasts = present\n",
    "        self.conditioned = True\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask\n",
    "    \n",
    "    def create_paddding_mask(self, inp):\n",
    "        padded = tf.cast(tf.math.equal(inp, 0), tf.float32)\n",
    "        return padded[:, tf.newaxis, tf.newaxis, :]\n",
    "    \n",
    "    def create_mask(self, x):\n",
    "        matrix_dim = tf.shape(x)[1]\n",
    "        \n",
    "        mask_ahead = self.create_look_ahead_mask(matrix_dim)\n",
    "        mask_padd = self.create_paddding_mask(x)\n",
    "        \n",
    "        mask = tf.maximum(mask_padd, mask_ahead)\n",
    "        return mask\n",
    "    \n",
    "    def call(self, input):\n",
    "        presents = []\n",
    "        \n",
    "        assert len(self.pasts) == self.num_layers\n",
    "        \n",
    "        mask = self.create_mask(input)\n",
    "        x = self.PosEmb(input)\n",
    "        \n",
    "        for DecoderB, past in zip(self.DecoderBlocks, self.pasts):\n",
    "            x, present = DecoderB((x, past, mask))\n",
    "            presents.append(present)\n",
    "            \n",
    "        x = self.Norm(x)\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = tf.reshape(x, [-1, self.d_model])\n",
    "        x = tf.matmul(x, self.PosEmb.get_token_embedding(), transpose_b=True)\n",
    "        logits = tf.reshape(x, [-1, seq_len, self.vocab])\n",
    "        return logits, presents\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        gc.collect()\n",
    "        x, y = data\n",
    "        x = tf.cast(x, tf.int32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred, _ = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        perplexity = tf.exp(loss)\n",
    "        \n",
    "        return {\"Loss\": loss, \"Perplexity\": perplexity}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "  \n",
    "        y_pred, _ = self(x, training=False)\n",
    "\n",
    "        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        \n",
    "        perplexity = tf.exp(loss)\n",
    "\n",
    "        return {\"Loss\": loss, \"Perplexity\": perplexity}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "Root = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_originalData = Root + \"/Dataset\"\n",
    "all_files = [os.path.join(path_originalData,file) for file in os.listdir(path_originalData) if '.txt' in file[-4:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix posible problems in Unicode Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ftfy import fix_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procesed_path = \"./Processed.txt\"\n",
    "writer = open(procesed_path, \"w\", encoding='utf-8')\n",
    "for file in all_files:\n",
    "    f = open(file, \"r\", encoding='utf-8')\n",
    "    writer.writelines([fix_text(line, normalization=\"NFKC\") for line in f.readlines()])\n",
    "    f.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = Counter()\n",
    "with open(procesed_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        token_count.update(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_path = \"./Counter.txt\"\n",
    "with open(counter_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    output = csv.writer(f, delimiter='\\t')\n",
    "    for word in token_count:\n",
    "        output.writerow([word, token_count[word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libreria que nos crea la codificación byte Pair Encoding (el vocab_size es a nuestra elección)\n",
    "Model_path = \"./BPE_Model\"\n",
    "vocab_size = 24512 #int(len(token_count)/4)\n",
    "\n",
    "spmcmd = '--input={spm_input} --model_prefix={spm_model} --input_format=tsv --vocab_size={vocab_size} --user_defined_symbols=[SEP],[BOS],[EOS] --hard_vocab_limit=false --model_type=bpe --pad_id=0 --unk_id=1 --bos_id=-1 --eos_id=-1 --pad_piece=[PAD] --unk_piece=[UNK]'.format(\n",
    "spm_input=counter_path, spm_model=Model_path, vocab_size=vocab_size)\n",
    "spm.SentencePieceTrainer.train(spmcmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = spm.SentencePieceProcessor()\n",
    "s.Load(Model_path + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 3\n",
    "EOS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "count = 0\n",
    "\n",
    "min_seq_len = 15\n",
    "max_seq_len = 128\n",
    "\n",
    "with open(procesed_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        encod = s.encode_as_ids(line)\n",
    "        if max_seq_len > len(encod) > min_seq_len:\n",
    "            dataset += [[[BOS]+ encod, encod + [EOS]]]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = (85 / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [X[0] for X in dataset]\n",
    "Y = [Y[1] for Y in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=1-train_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(max([len(x) for x in X_train]), max([len(x) for x in X_test])) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for i in range(len(X_train)):\n",
    "    X_train[i] += [0 for _ in range(max_len-len(X_train[i]))]\n",
    "    y_train[i] += [0 for _ in range(max_len-len(y_train[i]))]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for i in range(len(X_test)):\n",
    "    X_test[i] += [0 for _ in range(max_len-len(X_test[i]))]\n",
    "    y_test[i] += [0 for _ in range(max_len-len(y_test[i]))]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = tf.data.Dataset.from_tensor_slices((tf.constant(X_train), tf.constant(y_train))).batch(1, drop_remainder=True)\n",
    "#test_dataset  = tf.data.Dataset.from_tensor_slices((tf.constant(X_test), tf.constant(y_test))).batch(1, drop_remainder=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((tf.ragged.constant(X_train), tf.ragged.constant(y_train))).map(lambda x,y: (x,y)).padded_batch(BATCH_SIZE)\n",
    "test_dataset  = tf.data.Dataset.from_tensor_slices((tf.ragged.constant(X_test), tf.ragged.constant(y_test))).map(lambda x,y: (x,y)).padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 8\n",
    "num_heads = 8\n",
    "d_model = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = GPT2(num_layers, num_heads, d_model, vocab_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(y_true, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "    \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(y_true, y_pred)\n",
    "    \n",
    "    mask = tf.cast(mask, loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "\n",
    "    loss = tf.reduce_sum(loss, axis=1)\n",
    "    average_loss = tf.reduce_mean(loss / tf.reduce_sum(mask, axis=1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return tf.reduce_mean(loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = gpt2.fit(train_dataset, validation_data=test_dataset, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        return logits\n",
    "\n",
    "    values, _ = tf.nn.top_k(logits, k=k)\n",
    "    min_values = values[:, -1]\n",
    "\n",
    "    return tf.where(\n",
    "        logits < min_values,\n",
    "        tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "        logits\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(logits, temperature=1, top_k=50):\n",
    "    logits = logits[:, -1, :]/tf.cast(temperature, tf.float32)\n",
    "    #print(logits)\n",
    "    logits = top_k_logits(logits, k=top_k)\n",
    "    #print(logits)\n",
    "    sample = tf.random.categorical(logits, num_samples=1, dtype=tf.int32)\n",
    "    #sample = tf.nn.softmax(logits)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puta = ''\n",
    "def generator_text(initial_sentence, model, seq_len, temp=1, k=10, clear_status=True):\n",
    "    global puta\n",
    "    if clear_status:\n",
    "        gpt2.reset_past_status()\n",
    "\n",
    "    context = tf.expand_dims(([BOS] + s.encode_as_ids(initial_sentence)), 0)\n",
    "    prev = context\n",
    "    output = context\n",
    "    j = 0\n",
    "    for i in range(seq_len):\n",
    "        logits = model.generate_next_output(prev)\n",
    "        puta = logits\n",
    "        #print(\"original\", logits)\n",
    "        sample = extract_data(logits)\n",
    "        #print(sample)\n",
    "        if tf.equal(sample, EOS):\n",
    "            print(j, \"END\")\n",
    "            break\n",
    "        output = tf.concat([output, sample], axis=-1)\n",
    "        prev = sample\n",
    "        j += 1\n",
    "    print(output)\n",
    "    result = tf.squeeze(output, axis=0)\n",
    "    pred = [int(i) for i in result]\n",
    "    generated_seq = s.decode_ids(pred[1:])\n",
    "    generated_seq = generated_seq.replace(\"[SEP]\", \"\").strip()\n",
    "    generated_seq = ' '.join(generated_seq.split())\n",
    "    return generated_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Initial_Sentence = \"i have a friend called \"\n",
    "generator_text(Initial_Sentence, gpt2, 50, temp=0.7, k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss([y_train[0]],  out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "Loss([y_train[0][:4]], out[:, :4, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss([y_train[0][:]], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [x for x in range(98)]\n",
    "l2 = [0 for x in range(98)]\n",
    "m = tf.math.logical_not(tf.math.equal(tf.constant([y_train[0]]), 0))\n",
    "tf.constant([y_train[0]])[m]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.reset_past_status()\n",
    "out = gpt2.generate_next_output(tf.constant([X_train[0]]))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.argmax(out, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant([X_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant([y_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in train_dataset.take(1):\n",
    "    print(ex[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = next(train_dataset.as_numpy_iterator())[0]\n",
    "Y = next(train_dataset.as_numpy_iterator())[1]\n",
    "print(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.math.logical_not(tf.math.equal(a, 0))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tf.shape(a)[0]\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = a[m]\n",
    "np.reshape(x, (batch, 98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
